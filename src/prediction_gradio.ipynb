{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5592f89-8260-4853-938c-6724f7fa300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85fd0d55-10a5-407e-bdd9-c16d8ead9053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from patchify import patchify, unpatchify\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.saving import register_keras_serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d6077a6-5a27-4316-81ec-55fcee460f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "class DiceFocalLoss(Loss):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, dice_weight=0.5, focal_weight=0.5, \n",
    "                 smooth=1e-6, from_logits=False, name=\"dice_focal_loss\", reduction = 'sum_over_batch_size'):\n",
    "        super(DiceFocalLoss, self).__init__(name=name, reduction=reduction)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.dice_weight = dice_weight\n",
    "        self.focal_weight = focal_weight\n",
    "        self.smooth = smooth\n",
    "        self.from_logits = from_logits\n",
    "        \n",
    "    def dice_loss(self, y_true, y_pred, use_true_union=False):\n",
    "        \"\"\"\n",
    "        Calculate Dice Loss for multiclass segmentation.\n",
    "        \n",
    "        Args:\n",
    "            y_true: Ground truth labels (batch_size, height, width, num_classes)\n",
    "            y_pred: Predicted probabilities (batch_size, height, width, num_classes)\n",
    "            use_true_union: If True, uses A+B-A∩B formula; if False, uses A+B (Sørensen-Dice)\n",
    "        \"\"\"\n",
    "        # Calculate intersection by summing over spatial dimensions (axis 1,2) for each class\n",
    "        intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])  # Shape: (batch, num_classes)\n",
    "        \n",
    "        if use_true_union:\n",
    "            # True set theory union: A + B - A∩B\n",
    "            sum_sets = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "            union = sum_sets - intersection\n",
    "        else:\n",
    "            # Sørensen-Dice coefficient: A + B (more commonly used in segmentation)\n",
    "            union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "        \n",
    "        # Dice coefficient for each class and batch\n",
    "        dice_coeff = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        # Average across classes and batches\n",
    "        dice_loss = 1.0 - tf.reduce_mean(dice_coeff)\n",
    "        return dice_loss\n",
    "    \n",
    "    def focal_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate Focal Loss for multiclass segmentation.\n",
    "        \n",
    "        Args:\n",
    "            y_true: Ground truth labels (batch_size, height, width, num_classes)\n",
    "            y_pred: Predicted probabilities (batch_size, height, width, num_classes)\n",
    "        \"\"\"\n",
    "        # Clip predictions to prevent log(0)\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        # Standard cross entropy\n",
    "        ce_loss = -y_true * tf.math.log(y_pred)\n",
    "        \n",
    "        # For focal loss, p_t is simply the predicted probability of the true class\n",
    "        # Since y_pred is already probabilities and y_true is one-hot encoded:\n",
    "        p_t = tf.reduce_sum(y_true * y_pred, axis=-1, keepdims=True)  # Extract prob of true class\n",
    "        \n",
    "        # Apply focal loss modulation: (1-p_t)^gamma\n",
    "        focal_weight = tf.pow((1 - p_t), self.gamma)\n",
    "        \n",
    "        # Apply class weighting (alpha) and focal weight\n",
    "        focal_loss = self.alpha * focal_weight * ce_loss\n",
    "        \n",
    "        # Return mean loss\n",
    "        return tf.reduce_mean(focal_loss)\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate combined Dice + Focal Loss.\n",
    "        \"\"\"\n",
    "        # Convert logits to probabilities if needed\n",
    "        if self.from_logits:\n",
    "            y_pred = tf.nn.softmax(y_pred, axis=-1)\n",
    "        \n",
    "        # Calculate individual losses\n",
    "        dice_loss_val = self.dice_loss(y_true, y_pred)\n",
    "        focal_loss_val = self.focal_loss(y_true, y_pred)\n",
    "        \n",
    "        # Combine losses with weights\n",
    "        total_loss = (self.dice_weight * dice_loss_val + \n",
    "                     self.focal_weight * focal_loss_val)\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "934637c1-f2fe-461f-b989-f9de4ed23f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "def multiclass_jaccard_index(y_true, y_pred, smooth=1e-6):\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    y_true_flatten = K.flatten(y_true)\n",
    "    y_pred_flatten = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_flatten * y_pred_flatten)\n",
    "    jaccard_index = (intersection + smooth) / (K.sum(y_true_flatten) + K.sum(y_pred_flatten) - intersection + smooth)\n",
    "    return jaccard_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6839bb7-0412-4a97-af34-da7d5b2e82c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Building': 0, 'Land (unpaved area)': 1, 'Road': 2, 'Vegetation': 3, 'Water': 4, 'Unlabeled': 5}\n",
      "{'Building': (60, 16, 152), 'Land (unpaved area)': (132, 41, 246), 'Road': (110, 193, 228), 'Vegetation': (254, 221, 58), 'Water': (226, 169, 41), 'Unlabeled': (155, 155, 155)}\n"
     ]
    }
   ],
   "source": [
    "saved_model = load_model('../model/semantic-segmentation-aerial-unet-v0.keras')\n",
    "INPUT_CLASS_FILE_PATH = os.path.join('../data', 'training_data_class_map.pkl')\n",
    "\n",
    "with open(INPUT_CLASS_FILE_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "label_map = data['label_map']\n",
    "class_labels_rgb = data['class_labels_rgb']\n",
    "\n",
    "print(label_map)\n",
    "print(class_labels_rgb)\n",
    "\n",
    "class_rgb = {label_map.get(k): class_labels_rgb.get(k) for k in label_map.keys()}\n",
    "label_map_rev = {v: k for k, v in label_map.items()}\n",
    "\n",
    "\n",
    "def pad_to_nearest_multiple_patchsize(image, patch_size):\n",
    "    height, width = image.shape[:2]\n",
    "    pad_h = (patch_size - height % patch_size) % patch_size\n",
    "    pad_w = (patch_size - width % patch_size) % patch_size\n",
    "    padded = np.pad(image, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "    return padded, height, width\n",
    "\n",
    "def unpad_to_original_size(padded_image, original_height, original_width):\n",
    "    if padded_image.ndim == 2:\n",
    "        return padded_image[:original_height, :original_width]\n",
    "    return padded_image[:original_height, :original_width, :]\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_rgb_colors(class_rgb, mask):\n",
    "    num_classes = len(class_rgb)\n",
    "    mask = mask.astype(np.int32)\n",
    "    lut = np.zeros((num_classes, 3), dtype=np.uint8)\n",
    "\n",
    "    for cl_id, color in class_rgb.items():\n",
    "        lut[cl_id] = color\n",
    "\n",
    "    rgb_mask = lut[mask]\n",
    "\n",
    "    return rgb_mask   \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9799bab1-4a90-482c-81a8-684b4cb0a1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(image, PATCH_SIZE=256):\n",
    "    # image = cv2.imread(image_path)\n",
    "    # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    padded_image, height, width = pad_to_nearest_multiple_patchsize(image, PATCH_SIZE)\n",
    "    padded_image = padded_image / 255.0\n",
    "    ph, pw = padded_image.shape[:2]\n",
    "    im_patches = patchify(padded_image, (PATCH_SIZE, PATCH_SIZE, 3), step=PATCH_SIZE)\n",
    "    num_i, num_j = im_patches.shape[:2]\n",
    "    return im_patches, num_i, num_j, height, width, ph, pw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70553c86-d740-4145-a65b-5a5cf5054644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_rgb_colors(class_rgb, mask):\n",
    "    num_classes = len(class_rgb)\n",
    "    mask = mask.astype(np.int32)\n",
    "    lut = np.zeros((num_classes, 3), dtype=np.uint8)\n",
    "\n",
    "    for cl_id, color in class_rgb.items():\n",
    "        lut[cl_id] = color\n",
    "\n",
    "    rgb_mask = lut[mask]\n",
    "\n",
    "    return rgb_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45d9805b-e9e4-4e76-8e6e-5a34b0bb7c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_filepath):\n",
    "    # print(type(image_filepath))\n",
    "    im_patches, num_i, num_j, height, width, ph, pw = preprocessing_pipeline(image_filepath)\n",
    "    im_shape = im_patches.shape[:-1]\n",
    "    mask_patches = np.zeros(im_shape)\n",
    "    for i in range(num_i):\n",
    "        for j in range(num_j):\n",
    "            prediction = saved_model.predict(im_patches[i, j, :, :, :])\n",
    "            predicted_image = np.argmax(prediction, axis=3)\n",
    "            mask_patches[i, j, :, :, :] = predicted_image\n",
    "    patches_reshaped = mask_patches.squeeze(axis=2) \n",
    "    unpatched_mask = unpatchify(patches_reshaped, (ph, pw))\n",
    "    reconstructed_mask = unpad_to_original_size(unpatched_mask, height, width)\n",
    "    rgb_mask = convert_to_rgb_colors(class_rgb, reconstructed_mask)\n",
    "    return rgb_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a0dc007-d1dd-453f-814c-f237d3d9b658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7878\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7878/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 809ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    title = gr.Markdown(\"\"\"<span style=\"font-size: 24px; font-weight: bold;\">Semantic Segmentation on Satellite Images</span>\"\"\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"Input Image\")\n",
    "            in_image = gr.Image(type=\"numpy\", image_mode='RGB')\n",
    "            btn = gr.Button('Submit')\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"Segmentation Result\")\n",
    "            out_image = gr.Image()\n",
    "        \n",
    "    btn.click(predict, inputs=in_image, outputs=out_image)\n",
    "\n",
    "demo.launch()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a416cf53-5e8c-44c7-81cc-6a310d2b506b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-ml",
   "language": "python",
   "name": "tf-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
